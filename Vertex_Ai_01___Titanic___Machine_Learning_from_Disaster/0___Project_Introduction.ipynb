{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Titanic - Machine Learning from Disaster : Project Introduction](#1)\n",
    "    * [Objective](#2)\n",
    "    * [Understand the Problem](#3)\n",
    "    * [Framing an ML problem](#4)\n",
    "    * [Table of Contents](#5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)** : `Project Introduction` <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objective** <a id=\"2\"></a>\n",
    "\n",
    "* The objective of this project is to apply `Machine Learning` (ML) techniques to predict survival rates on the `Titanic`, leveraging `Vertex AI` on Google Cloud Platform (GCP). \n",
    "\n",
    "* The goal is to develop robust models that can accurately predict whether a passenger survived the disaster, thereby providing insights into factors influencing `survival rates`. \n",
    "\n",
    "* This project aims to demonstrate proficiency in applying `ML best practices` and utilizing GCP's ML capabilities, using `Vertex AI`.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understand the Problem** <a id=\"3\"></a>\n",
    "\n",
    "![Image](titanic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **State the goal**\n",
    "\n",
    "* The goal of participating in the \"`Titanic - Machine Learning from Disaster`\" competition on Kaggle is to predict the survival rates of passengers aboard the `RMS Titanic` during its infamous maiden voyage. \n",
    "\n",
    "* Specifically, the task involves analyzing historical passenger data to determine whether they survived the disaster or not.\n",
    "\n",
    "* This challenge requires applying Machine Learning techniques to understand patterns within the data that could indicate factors influencing `survival` chances, such as age, gender, class, and presence of siblings/spouses.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clear use case for ML**\n",
    "\n",
    "* In the context of the `Titanic - Machine Learning from Disaster` competition on [Kaggle](https://www.kaggle.com/), applying machine learning (ML) is a specialized choice, especially considering the nature of the problem at hand. \n",
    "\n",
    "* The task involves predicting survival rates of passengers aboard the `Titanic` based on various features such as age, sex, passenger class, etc. \n",
    "\n",
    "* This scenario presents several considerations regarding the appropriateness of using ML over traditional methods.\n",
    "\n",
    "##### **Quality Assessment**\n",
    "\n",
    "* Firstly, assessing the quality improvement potential is crucial. Traditional `Statistical Analysis` or even simple `Logistic Regression` could serve as a baseline model. \n",
    "\n",
    "* These methods might already provide a decent prediction accuracy, especially if the dataset is relatively clean and well-understood. \n",
    "\n",
    "* However, ML models like `Decision Trees`, Random Forests, or `Gradient Boosting Machines` (GBMs) can potentially offer significant improvements in predictive power due to their ability to capture complex patterns and interactions between `Variables`.\n",
    "\n",
    "##### **Cost and Maintenance**\n",
    "\n",
    "* Regarding Cost and Maintenance, ML solutions can be resource-intensive. Training sophisticated models like neural networks requires substantial computational resources, which might not be justified unless the expected improvement in performance justifies the increased complexity and cost. \n",
    "\n",
    "* Though ML models often require ongoing monitoring and tuning to maintain performance as new data becomes available or as the underlying assumptions change, that is not needed here since the data comes from a `Historical event` that has long concluded.\n",
    "\n",
    "##### **Conclusion**\n",
    "\n",
    "* For the Titanic competition, if the traditional statistical methods or simpler ML techniques like Logistic Regression or Decision Trees yield satisfactory results, then further exploration into more complex ML models might not be warranted. \n",
    "\n",
    "* However, if after optimizing these simpler approaches, you still see room for improvement, exploring more advanced ML models could be beneficial. \n",
    "\n",
    "* It's essential to weigh the potential gains against the increased complexity, cost, and maintenance requirements of ML solutions.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predictive ML and data**\n",
    "\n",
    "* The data required to train a model using a `Predictive ML` approach is available. \n",
    "\n",
    "* The [Titanic dataset](https://www.kaggle.com/competitions/titanic/data) provided by Kaggle contains detailed information about each passenger, including their survival status, age, sex, passenger class, fare paid, and other relevant details. \n",
    "\n",
    "* This dataset is sufficient for training a predictive ML model aimed at predicting survival outcomes. \n",
    "\n",
    "* The dataset consists of `891 rows`, each representing a passenger, and `12 columns` recording various attributes such as passenger ID, passenger class, age, gender, number of siblings/spouses aboard, number of parents/children aboard, fare, ticket number, cabin number, port of embarkation, and survival status. \n",
    "\n",
    "* This comprehensive dataset allows for effective `Feature Engineering` and `Model Training` to achieve the goal of predicting passenger survival rates.\n",
    "\n",
    "##### **Predictive power**\n",
    "\n",
    "* `Predictive Power` refers to how well a feature can predict the outcome variable (in this case, survival). Features with strong correlations to the outcome are more likely to contribute positively to your model's performance.\n",
    "\n",
    "* `Feature Selection` is identifying which features have the highest predictive power is essential. This process involves understanding the relationship between each feature and the target variable.\n",
    "\n",
    "* To identify High-Predictive Features:\n",
    "    * Perform `Data Profiling` to understand its structure and Identify potential Features that might be relevant to predicting passenger survival. Look for features that logically seem to influence survival rates, such as age, gender, class, fare paid, etc.\n",
    "    * Use statistical measures like Pearson Correlation Coefficient to quantify the linear relationship between each feature and the target variable (Survived). A higher absolute value indicates a stronger correlation.\n",
    "    * Implement machine learning models that can rank features by importance. Tree-based models like Random Forests or XGBoost can provide insights into which features are most influential in making predictions.\n",
    "    * Leverage your knowledge of the Titanic disaster to understand which features might be particularly relevant. For instance, being in a higher class or having a ticket price above a certain threshold might correlate with survival chances.\n",
    "    * Experiment with different subsets of features to see how they affect your model's performance. This iterative process helps refine your feature selection strategy.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Framing an ML problem** <a id=\"4\"></a>\n",
    "\n",
    "### **Ideal Outcome and the Model's Goal**\n",
    "\n",
    "* The ideal outcome for the \"`Titanic - Machine Learning from Disaster`\" competition on Kaggle is to accurately predict whether a passenger survived the sinking of the Titanic or not. \n",
    "\n",
    "* This prediction is based on various features available in the dataset, such as passenger class, sex, age, number of siblings/spouses aboard, number of parents/children aboard, and fare paid. \n",
    "\n",
    "* The ultimate goal of the model is to maximize the accuracy of these predictions, thereby providing insights that could potentially save lives in future maritime disasters.\n",
    "\n",
    "##### **Identify the output**\n",
    "\n",
    "* In the \"`Titanic - Machine Learning from Disaster`\" competition on Kaggle, the goal is to predict whether a passenger survived the sinking of the Titanic or not. This makes the problem a `Binary Classification` task. Therefore, the output you need from your model is a `probability` that indicates the likelihood of `survival` for each passenger.\n",
    "\n",
    "* This output is crucial because it allows you to rank passengers according to their predicted chances of survival. By assigning probabilities, you can prioritize actions such as allocating limited lifeboats to those passengers deemed most likely to survive, thereby potentially increasing the overall survival rate among passengers.\n",
    "\n",
    "* The choice of model type for this task could vary depending on several factors, including the amount of available data, computational resources, and the complexity of the relationships between features and the target variable. However, given our expertise in Python and machine learning, we might consider using algorithms like Logistic Regression, Decision Trees, Random Forests, Gradient Boosting Machines (GBM), or Neural Networks. Each of these models has its strengths and weaknesses, but they are all capable of handling Binary Classification problems effectively.\n",
    "\n",
    "* For instance, Logistic Regression is straightforward and interpretable, making it a good starting point. Random Forests and GBMs offer higher accuracy and robustness against overfitting, especially with larger datasets. Neural Networks, particularly with deep learning approaches, can capture complex patterns but require more data and computational power.\n",
    "\n",
    "* Ultimately, the selection of the model type should be guided by the performance metrics on the validation set and the interpretability of the model, considering the real-world implications of your predictions.\n",
    "\n",
    "##### **Classification**\n",
    "\n",
    "* In this project, we will be Building and Training a `Binary Classification` model.\n",
    "\n",
    "### **Success Metrics**\n",
    "\n",
    "* In the context of the \"`Titanic - Machine Learning from Disaster`\" competition on Kaggle, defining success metrics involves understanding both the nature of the problem and the goals of the competition. \n",
    "\n",
    "* Given that the primary objective is to predict passenger survival based on various features, the success metrics should reflect the ability of the model to accurately classify passengers into those who survived and those who did not. \n",
    "\n",
    "* This makes it a classification problem, not a regression problem, despite initial impressions due to the presence of numerical features like age.\n",
    "\n",
    "##### **Success Metrics for Classification Problems**\n",
    "\n",
    "For classification problems, the most commonly used success metrics are:\n",
    "\n",
    "1. `Accuracy`: The proportion of true results (both true positives and true negatives) among the total number of cases examined. It measures the overall correctness of the predictions.\n",
    "\n",
    "1. `Precision`: The ratio of correctly predicted positive observations to the total predicted positives. High precision relates to low false-positive rate.\n",
    "\n",
    "1. `Recall` (Sensitivity): The ratio of correctly predicted positive observations to the all observations in actual class. High recall relates to low false-negative rate.\n",
    "\n",
    "1. `F1 Score`: The weighted average of Precision and Recall. It tries to balance the trade-off between precision and recall.\n",
    "\n",
    "##### **Why These Metrics?**\n",
    "\n",
    "* Accuracy provides a straightforward measure of how often the model is correct. However, it can be misleading if the classes are imbalanced, as seen in the Titanic dataset where the majority of passengers did not survive.\n",
    "\n",
    "* Precision and Recall offer insights into the model's performance across different aspects of the prediction task. Precision helps ensure that the model doesn't incorrectly predict too many survivors, while Recall ensures that the model captures as many actual survivors as possible.\n",
    "\n",
    "* F1 Score combines Precision and Recall into a single metric, providing a balanced view of the model's performance, especially useful in scenarios where the cost of false positives and false negatives differs.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents** <a id=\"5\"></a>\n",
    "\n",
    "### **1. Data Preparation**\n",
    "\n",
    "In this module we will `Prepare` our `Data`. Here, we will perform : \n",
    "\n",
    "1. **Data Collection :** The [dataset](https://www.kaggle.com/competitions/titanic/data) for `Titanic - Machine Learning from Disaster` is readily available at `Kaggle`. We will load this dataset into `Google Cloud Storage`.\n",
    "\n",
    "2. **Data Profiling :** \n",
    "\n",
    "3. **Data Pre-processing :**  \n",
    "\n",
    "4. **Data Validation :** \n",
    "\n",
    "'''\n",
    "\n",
    "### **2. Feature Engineering**\n",
    "\n",
    "'''\n",
    "\n",
    "### **3. AutoML Prototyping**\n",
    "\n",
    "* Serves as a Benchmark\n",
    "\n",
    "'''\n",
    "\n",
    "### **4. Custom Training**\n",
    "\n",
    "'''\n",
    "\n",
    "### **5. MLOPs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
